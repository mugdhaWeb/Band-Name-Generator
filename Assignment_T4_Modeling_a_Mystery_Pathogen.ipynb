{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment T4: Modeling a Mystery Pathogen",
      "provenance": [],
      "collapsed_sections": [
        "cYC7Itksfpz_",
        "JnD5A6FAfBul"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mugdhaWeb/Band-Name-Generator/blob/main/Assignment_T4_Modeling_a_Mystery_Pathogen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f9oU-Fzcnmf"
      },
      "source": [
        "# **Assignment T4: Modeling a Mystery Pathogen**\n",
        "\n",
        "*Assignment written by Aleks Jovcic, Alex Tsun, and Adam Pahlavan*\n",
        "\n",
        "In the following assignment, you will conduct your first foray into Machine Learning with Multiple Linear Regression, Feature Manipulation, and LASSO Regression!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1VGOd2-czOa"
      },
      "source": [
        "---\n",
        "\n",
        "## Overview\n",
        "In this assignment you will be working with a simulated dataset that we've created called `T4_data.txt`. This simulated dataset represents the fictional scenario of the mysterious **Disease Q** spreading through populations. You are taking the role of a data scientist who has been tasked with gleaning from this data information that could be beneficial in the fight against the mysterious pathogen.\n",
        "\n",
        "Each row in the data is a single **data point**, which represents a **population region**, and contains values for each of 4 columns. You are going to be writing code throughout this assignment to try and predict the label column $Y$, based on the three feature columns, $X_1, X_2,$ and $X_3$. Remember that this is the crux of Machine Learning: predicting **labels** based on **features**.\n",
        "\n",
        "Let's talk about what each of these columns represent:\n",
        "* $Y$: Represents the percent of the population that has been infected with Disease Q 10 days after the first case.\n",
        "* $X_1$: Represents the average number of cars passing a common toll booth on the border of this population region per minute over this 10 day period. If the value is positive, that means more cars are coming *into* the region than are leaving, and if it is negative, the opposite is true. *(E.g.: a value of 7.32 means that on average, 7.32 more cars drove into this region than out of it, per minute).*\n",
        "* $X_2$: Represents the change in the average amount of people in outdoor spaces at peak hours since the first case. A positive value means, generally, the amount of people outside has increased. *(E.g.: A value of -3.09 means that there are, on average, 3.09 less people in a given outdoor space at the end of these 10 days, then there were at the start).*\n",
        "* $X_3$: Represents the average number of one-time-use masks being produced per hour in this region. If the number is positive, it means they are being produced in excess, and if it is negative, that means that they are being used faster than they are being produced. *(E.g: A value of exactly 0, means that, on average, masks are being used and expended as fast as they are being produced).*\n",
        "\n",
        "The data we have provided is 100,000 rows of data points, each of which is a region. Over the course of this assignment, you are going to train various Machine Learning models to learn from this data, and be able to make predictions as to how much of a population will be infected based on the practices of the area.\n",
        "\n",
        "Obviously, this data is a simplified version of how such things work, and a Machine Learning model will rarely work as well as they will in this assignment. However, hopefully the real-world connections to this are apparent. The techniques we are going to use are real techniques employed in Machine Learning all the time, and are generalizable to so many topics, issues, and experiments. What we hope to demonstrate, above all else, is the power of Machine Learning, and how that power is in your hands.\n",
        "\n",
        "*Digression: You may notice throughout the assignment parts marked as \"Digressions\", like this one. This just means these bits of information are not necessary to complete the assignment, but you may find them interesting.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqbaXa0jukjV"
      },
      "source": [
        "# Before Starting: Save a Copy in Drive\n",
        "\n",
        "BEFORE YOU DO ANYTHING ELSE, make sure you save a copy of this Colab Notebook in your Google Drive by clicking *File -> Save a Copy in Drive*. By default, the Colab will be saved in *My Drive -> Colab Notebooks*. Since the original link is view-only, any edits you make will not be saved after you close the tab. Please refer to the image below:\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=download&id=1a2maN-szhK79yuy9zFjqlSrUdyzJxL4g\" width=\"300\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2FY3NUxs7HW"
      },
      "source": [
        "---\n",
        "\n",
        "# Problem 1: Multiple Linear Regression\n",
        "\n",
        "In Problem 1, you will be performing Multiple Linear Regression as we've shown in lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ae07BxaDEo4"
      },
      "source": [
        "## **(1a) Hypothesize the Relationship**\n",
        "\n",
        "Before we go about manipulating our data, we need to make a hypothesis as to how it is related. For each of the 3 features ($X_1$, $X_2$, and $X_3$), hypothesize what kind of relationship it has with the label ($Y$). Does it have a positive, polynomial relationship? What about a negative, linear relationship? In other words, what impact do you think each of these features has on the label (what percentage of the population is infected with Disease Q)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r8DdKBXEI6j"
      },
      "source": [
        "<font color='CC6600'> *Write your text answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jaa0S1UUAZC5"
      },
      "source": [
        "## **(1b) Setup and Loading Data**\n",
        "\n",
        "To start, running the following two cells to import most of the necessary libraries for this assignment (there are some libraries it will be on you to import yourself later) and load in the dataset. After running the second cell, the data will be in the form of a numpy array called `data`.\n",
        "\n",
        "Note that `data` is a numpy array, and thus we do not have the luxury of column names. Know that columns 0, 1, 2, and 3 represent $X_1$, $X_2$, $X_3$, and $Y$ respectively.\n",
        "\n",
        "*Digression: You'll notice that we first use `pd.read_csv` to load in the data. This is because numpy doesn't have the capability to load data from a URL like Pandas does. However, we want our data in the form of a numpy array for ease of usage later. For this reason, we then convert it to a numpy array using `data.to_numpy()`. This is a wonderful example of the libraries we're using playing nicely with each other.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKF_O5Wx69"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4sWX3CIA-0m"
      },
      "source": [
        "DATA_URL = 'https://drive.google.com/uc?export=download&id=1Kk4GGBc9GYD3yJv9gNAuHznCKbYn4AU5'\n",
        "\n",
        "data = pd.read_csv(DATA_URL, names=[\"X1\", \"X2\", \"X3\", \"y\"], header=0)\n",
        "\n",
        "data = data.to_numpy()\n",
        "\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFYkmDOjtAWF"
      },
      "source": [
        "## **(1c) Splitting into Train, Val, and Test**\n",
        "\n",
        "Split `data` into `X_train`, `X_val`, `X_test`, `y_train`, `y_val`, and `y_test` in ratios of 80%, 10%, and 10%.\n",
        "\n",
        "*Hint: Use the numpy function [np.split()](https://numpy.org/devdocs/reference/generated/numpy.split.html)*\n",
        "\n",
        "*Digression: You may notice (and be annoyed by) the fact that X is capitalized while y is not. The reason for this is that under ML convention, X is capitalized because it is a matrix, whereas y is lowercase because it is a vector.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmC2dIT2DZqK"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ohlt8A7KnK5"
      },
      "source": [
        "## **(1d) Creating and Fitting a Multiple Linear Regression Model**\n",
        "\n",
        "Create a Multiple Linear Regression model called `model`, train it on `X_train` and `y_train`, and print the weights and bias. You will first need to import `LinearRegression` from `sklearn.linear_model`.\n",
        "\n",
        "*Hint: If you are unsure how to perform these tasks, try reading the documentation for sklearn's [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-mq1IekLugl"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1Ciosf3dDi0"
      },
      "source": [
        "## **(1e) Giving the Y-hat Equation**\n",
        "\n",
        "Based on the weights and bias you recieved from your Linear Regression model, write an equation for $\\hat{Y}$ in $\\LaTeX$. Round your weights to two decimal places.\n",
        "\n",
        "*Hint: To find out how to write $\\hat{Y}$ in $\\LaTeX$, double click on this text box!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddicNCLxdFrh"
      },
      "source": [
        "<font color='CC6600'> *Write your $\\LaTeX$ statement here.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z1OCXXrdMpz"
      },
      "source": [
        "## **(1f) Finding Train and Val MSE**\n",
        "\n",
        "Find and print the Mean Squared Error (MSE) for `model`'s prediction on the Train set, as well as the MSE for `model`'s prediction on the Val set. You will first need to import `mean_squared_error` from `sklearn.metrics`.\n",
        "\n",
        "*Hint: If you are unsure how to perform these tasks, try reading the documentation for sklearn's [mean_squared_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) function.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvzTb-ikKZVY"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mamC8FY4JJJ2"
      },
      "source": [
        "If you did the above calculation correctly, you should notice that the MSE are not terrible but also have a large margin for improvement. As responsible data scientists, it is our job to figure out why this is the case and if we can do better. To start, we'll need to investigate our data a bit. We can start doing this by making what are called **marginal plots**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP80zfO8uS1L"
      },
      "source": [
        "## **(1g) Marginal Plots**\n",
        "\n",
        "In T3, we were able to make a scatter plot of our two variables directly and describe their approximate relationship. Unfortunately, in this assignment, we have 3 features and 1 label (called 3-dimensional data). Thus, it is difficulty (nigh-impossible) to do anything like graphing all our features vs. all our data.\n",
        "\n",
        "However, what we can do is make what are called **marginal plots**. Marginal plots are scatter plots comparing a single feature to the label. This allows us to see if there are any potential underlying behaviors occuring with any of our individual features.\n",
        "\n",
        "In the three cells below, make and show three scatter plots comparing each of the three features ($X_1$, $X_2$, and $X_3$) to $Y$. Each plot should have a descriptive title and axes labels. In the text boxes, give a qualitative description of the relationship you see in the scatter plots. You will have to import `matplotlib.pyplot`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGJ84sqNFsce"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWX_uSrCK0JB"
      },
      "source": [
        "<font color='CC6600'> *Write your text answer here*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnhYlniZHrMH"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67u90MirK3UG"
      },
      "source": [
        "<font color='CC6600'> *Write your text answer here*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5O7VgwiH5x1"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs2i-GBhK4f_"
      },
      "source": [
        "<font color='CC6600'> *Write your text answer here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6MeJwVUK69s"
      },
      "source": [
        "*Digression: You may notice that some of the data points for variable Y go above 100%. This is to represent a scenario in which the immediate population of a region actually increases in the given 10 days (perhaps it is a popular vacation destination and a large population flocks there despite the ongoing spread of Disease Q).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8Ar3gLsL7W8"
      },
      "source": [
        "---\n",
        "\n",
        "# Problem 2: Transforming the Data\n",
        "\n",
        "If you created your marginal plots in Problem 1 correctly, you should have seen that $X_1$ doesn't seem to have a linear relationship with $Y$; in fact, the scatterplot is curved and resembles a higher order poynomial degree relationship! On the other hand, the marginal plots of $X_2$ and $X_3$ more-so resemble linear relationships, albeit with moderate heteroskedasticity.\n",
        "\n",
        "We will now investigate the $X_1$ variable further  and see if anything can be done to improve our model. As we saw, the val MSE we got wasn't great. Perhaps this is a way to fix that!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSrIFazetNAW"
      },
      "source": [
        "## **(2a) Feature Manipulation (Making X1 Polynomial)**\n",
        "\n",
        "The first thing we will do is **transform** our data by making X polynomial. Create a new numpy array `trans_X`, which contains the following columns (in order from left to right):\n",
        "\n",
        "> $X_1$, $X_1^2$, $X_1^3$, $X_1^4$, $X_1^5$, $X_2$, $X_3$\n",
        "\n",
        "We don't necessarily know if X is going to have behavior all the way up to the 5th degree, but we're generating columns up until that point in the hopes of finding out exactly what kind of polynomial relationship it has!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myqPmEu_L3n_"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB3fQkZ-uMAo"
      },
      "source": [
        "## **(2b) Marginal Plots**\n",
        "\n",
        "Once again, we are going to be making marginal plots. This time, however, we are going to make **5** plots, one for each degree of $X_1$. Do this in each of the following code blocks, and enter a written answer once again giving a brief, qualitative description of the graph.\n",
        "\n",
        "As before, show each graph and give them each a descriptive title and axes labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Oyli3k1MTok"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBSfzOj1PxDC"
      },
      "source": [
        "<font color='CC6600'> *Write your text answer here*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fgmgkvZMcrM"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydNIeu9CP2yJ"
      },
      "source": [
        "<font color='CC6600'> *Write your text answer here*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltH7g4mFMfzQ"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lb--Ad-P3dh"
      },
      "source": [
        "<font color='CC6600'> *Write your text answer here*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycj8jkFsMrKC"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZElkrhBLP4D1"
      },
      "source": [
        "<font color='CC6600'> *Write your text answer here*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xff-ascIbJMJ"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GabOQgvNP4uY"
      },
      "source": [
        "<font color='CC6600'> *Write your text answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G96_hkbDP-SF"
      },
      "source": [
        "If you made these marginal plots correctly, you should see that the untransformed $X_1$ looks the same, but that each higher degree transformation looks mostly linear (albeit getting stranger as they get higher). There's not too much we can glean from this about the exact behavior of $X_1$ unfortunately, however it does seem to support our suspicions that there is some higher-degree behavior underlying the relationship between $X_1$ and $Y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_RPqO4UPwRz"
      },
      "source": [
        "## **(2c) Standardizing the Data**\n",
        "\n",
        "The next step in transforming our data is going to be to **standardize** it. This is the process by which we put our features into **z-score space**. This is a very important process because it makes sure that differing scales of units do not become an issue when we apply the LASSO penalty in problem 3.\n",
        "\n",
        "Create a new numpy array `trans_standard_X`, containing the standardized version of `trans_X`.\n",
        "\n",
        "*Digression: You may be wondering why we aren't transforming $Y$ as well. While there would be nothing wrong with transforming $Y$ to Z-space as well, it would not be necessary in the same way that standardizing the $X$ variables would be. When it comes to Ridge and LASSO regression, all the matters is that the relative scale of the $X$ variables is the same; that way, the Ridge and LASSO penalties are applied to all input variables equally, and no input term gets a boost in importance just because its scale happened to be bigger.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4td769jPrq5"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhsNl8ERuv8F"
      },
      "source": [
        "## **(2d) Splitting Into Train, Val, and Test**\n",
        "\n",
        "Split `trans_standard_X` into `trans_standard_X_train`, `trans_standard_X_val`, and `trans_standard_X_test` in ratios of 80%, 10%, and 10%.\n",
        "\n",
        "*Hint: Use the numpy function [np.split()](https://numpy.org/devdocs/reference/generated/numpy.split.html)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPwOOICyQFDK"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzZUsqygt0Zu"
      },
      "source": [
        "## **(2e) Creating and Fitting a NEW Multiple Linear Regression Model**\n",
        "\n",
        "Create a NEW Multiple Linear Regression model called `trans_model`, train it on `trans_standard_X_train` and `y_train`, and print the weights and bias.\n",
        "\n",
        "*Hint: If you are unsure how to perform these tasks, try reading the documentation for sklearn's [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeyjZLF1QKjY"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir7QbRrsck1y"
      },
      "source": [
        "## **(2f) Giving the Y-hat Equation**\n",
        "\n",
        "Based on the weights and bias you recieved from your new Linear Regression model, write a NEW equation for $\\hat{Y}$ in $\\LaTeX$. Round your weights to two decimal places.\n",
        "\n",
        "*Hint: To find out how to write $\\hat{Y}$ in $\\LaTeX$, double click on this text box!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlHl-jDYcnY_"
      },
      "source": [
        "<font color='CC6600'> *Write a $\\LaTeX$ statement here.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97d7CPpnT9BA"
      },
      "source": [
        "If you fit your model correctly, you should notice that the weights for $X_1^4$ and $X_1^5$ round to 0! This likely means that $X_1$ has only a cubic relationship to $Y$, and not any degree higher. To verify that, let's find the Train and Val MSE again to see if it's any better with our transformed data. If they're sufficiently low, we can infer that our model is an accurate one, and that having cubic behavior is an accurate assessment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejeTUC3qegGl"
      },
      "source": [
        "## **(2g) Finding Train and Val MSE**\n",
        "\n",
        "Find and print the Mean Squared Error (MSE) for `trans_model`'s prediction on the transformed, standardized Train set, as well as the MSE for `trans_model`'s prediction on the transformed, standardized Val set.\n",
        "\n",
        "*Hint: If you are unsure how to perform these tasks, try reading the documentation for sklearn's [mean_squared_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) function.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9byPmRwgRumi"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YchxsyxKU-i-"
      },
      "source": [
        "If you've done everything right up until this point, you should find that the MSE's are much better than before! This is promising, and means that we have likely found a much better relationship between our features and our label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNu20Sy8deWD"
      },
      "source": [
        "---\n",
        "\n",
        "# [Optional Challenge] Problem 3: LASSO Regression\n",
        "\n",
        "*The following question is an optional challenge for assignment T4.*\n",
        "\n",
        "As we talked about in lecture, LASSO regression is a special type of multiple linear regression that penalizes high weights. How much high weights are penalized is based on a **hyperparameter** called **alpha**. Remember that a hyperparameter is something that we, the user, sets for a model, NOT something that the model learns on its own (those are called *parameters*). We're going to be using LASSO in this investigation for two reasons:\n",
        "\n",
        "* We want to see if there are better MSE's that arise from different values of alpha.\n",
        "* We want to use it to evaluate feature importance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2OBOQtltUwM"
      },
      "source": [
        "## **(3a) Creating and Fitting a LASSO Model With Default Alpha (1.0)**\n",
        "\n",
        "Create a LASSO Regression model called `lasso`, with the default alpha value of 1.0, train it on `trans_standard_X_train` and `y_train`, and print the weights and bias.\n",
        "\n",
        "*Hint: If you are unsure how to perform these tasks, try reading the documentation for sklearn's [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) class.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMs6rj-xdjM5"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w7sU1KbYQBK"
      },
      "source": [
        "You should notice that we get slightly different weights this way. Let's find the train and val MSE's to see if these different weights make any difference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYC7Itksfpz_"
      },
      "source": [
        "## **(3b) Finding Train and Test MSE**\n",
        "\n",
        "Find and print the Mean Squared Error (MSE) for `lasso`'s prediction on the transformed, standardized Train set, as well as the MSE for `lasso`'s prediction on the transformed, standardized Val set.\n",
        "\n",
        "*Hint: If you are unsure how to perform these tasks, try reading the documentation for sklearn's [mean_squared_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) function.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZc4-vVyforr"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWaYWu_zYfN8"
      },
      "source": [
        "If you did everything right, you should find that both MSE's are somewhat worse than they were in Problem 2(g). Let's see if this trend continues as alpha increases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnD5A6FAfBul"
      },
      "source": [
        "## **(3c) Trying Various Alpha Values**\n",
        "\n",
        "Try alpha values from 0 to 10 (inclusive), stepping in increments of 0.1, obtaining the weights, Train MSE, and Val MSE for each alpha value tried.\n",
        "\n",
        "Store this information in a new numpy array called `lasso_data`. Column 0 should be the alpha values, columns 1 through 7 (inclusive) should be the obtained weights, and columns 8 and 9 should be the Train and Val MSE's respectively.\n",
        "\n",
        "You might see the following UserWarning when you run your code:\n",
        "\n",
        "```\n",
        "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
        "  positive)\n",
        "```\n",
        "\n",
        "Don't worry about this, your code should run fine despite this warning.\n",
        "\n",
        "*Hint: Generate the array `lasso_data` first, with all the alpha values as a column and the other 10 values being filled with zeroes.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx8RW3X7f2XJ"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxfBnli5knyc"
      },
      "source": [
        "## **(3d) Plotting a LASSO Path**\n",
        "\n",
        "Plot a line graph with alpha on the x-axis, and weight on the y-axis, with a line for each of the features from our transformed X.\n",
        "\n",
        "As before, show the graph and give it a descriptive title and axes labels. This time, also give your graph a legend indicating which line is which feature.\n",
        "\n",
        "Also, write a text answer describing the LASSO path and any information you can glean from it. Consider the slopes of the paths, what value they converge to, which feature converges last, etc...\n",
        "\n",
        "*Hint: To plot multiple lines on one set of axes, simply call `plt.plot()` multiple times before calling `plt.show()`. To have a legend, pass in the parameter `label` to each call to `plt.plot()`, and call `plt.legend()` before you show the graph.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBqQRTgFgtWP"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMLmnXuSczOM"
      },
      "source": [
        "<font color='CC6600'> *Write your text answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmSgXmBotkol"
      },
      "source": [
        "## **(3e) Plotting Train and Val MSE**\n",
        "\n",
        "Plot Train and Val MSE on one graph against different values of alpha. As before, show the graph, give it a descriptive title and axes labels, and give your graph a legend indicating which line is which MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4vXMY6nnJkQ"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp5FACMdcMq8"
      },
      "source": [
        "You should notice that Train and Val MSE are closely linked! This is not too unusual. You'll also notice that the lowest MSE seems to be when alpha is equal to 0! This makes sense, based on what we noticed before when fitting LASSO with alpha set to 1.0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB1eArcyvCR_"
      },
      "source": [
        "## **(3f) Final LASSO Model**\n",
        "\n",
        "Given the information obtained from the MSE graph, pick an alpha value as your ideal value and justify your choice. Make one final LASSO model, passing in your ideal alpha value. Fit it on the transformed, standardized Train set, and print the weights and bias.\n",
        "\n",
        "*Hint: The ideal alpha will most likely not be 0! 0 has the lowest MSE, but there are other benefits to a higher alpha that might be worth a slightly higher MSE.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKKLsjD2o1C6"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtf1G1MF3uXf"
      },
      "source": [
        "<font color='CC6600'> *Write your text answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsd2XzvmgtjX"
      },
      "source": [
        "## **(3g) Finding Train, Val, and Test MSE**\n",
        "\n",
        "Find and print the Train and Val MSE one last time, this time with this final LASSO model. Also find the Test MSE (notice that we only looked at the Test Set *once*)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mn3wlzpqL0B"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MiYt36HfCMP"
      },
      "source": [
        "You'll notice that the Test MSE is slightly higher, but not incredibly so. This is likely due to just minor deviations in the data, and is nothing to worry about."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLxjcykOfPiQ"
      },
      "source": [
        "## **(3h) Giving the Y-hat Equation**\n",
        "\n",
        "Finally, based on the weights and bias you recieved from your final LASSO model, write an equation for $\\hat{Y}$ in $\\LaTeX$. Round your weights to two decimal places."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y_095B8fh2F"
      },
      "source": [
        "<font color='CC6600'> Write a $\\LaTeX$ statement here."
      ]
    }
  ]
}